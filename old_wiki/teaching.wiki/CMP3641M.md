# Welcome to the CMP3641M Wiki (Robotics part)

This is the Wiki in support of the CMP3641M module at the [University of Lincoln](http://www.lincoln.ac.uk)

## Desktop Machines

* To set up your Desktop computer to run the software relevant for this module head over to [[CMP3641M Computer Setup]]
  * __Note:__ The computers in CompLab C are ready for this, you can try this at home, but we neither guarantee that it works nor can we provide extensive support. 

## The Robots

* Find out everything about the real [[Turtlebots]]
* For Simulation, look [here](https://github.com/LCAS/teaching/blob/indigo-devel/uol_turtlebot_simulator/tutorial.md)

## Workshops

### Week 1: First Steps

This week, we'll mostly look at simulation and setting up our computers and working environments. You should

1. boot into Linux
1. make sure all the software is installed according to [[CMP3641M-Computer-Setup]]
1. get used to work with the terminal in Linux: a good starting point is [the official tutorial for beginners](https://help.ubuntu.com/community/UsingTheTerminal). _Please skip the bit about adding a new user._
1. familiarise yourself with the programming environment [Spyder](https://pythonhosted.org/spyder/)
1. get some very basic ROS commands going: the tutorials 1-6 form [the official ROS tutorials](http://wiki.ros.org/ROS/Tutorials) are a good starting point.
1. try the simulation following this [tutorial](https://github.com/LCAS/teaching/blob/indigo-devel/uol_turtlebot_simulator/tutorial.md). When you have the simulation up and running, get a demonstrator to record your achievement.

### Week 2: First steps with the real thing and ROS

1. Get your turtlebot from the demonstrators!
1. Read [[Turtlebots]] and particularly familiarise yourself with the [[Turtlebots#handling-your-turtlebot]]
1. In this workshop, we will be connecting to the turtlebots from our own desktop machines via the wireless network. Your turtlebots have an IP address assigned which you will be using to talk to them through ROS. First, try to login to the turtlebot as described in [[Turtlebots#connecting-to-the-robot]]. 
1. Start the main drivers of the robot as described in [[Turtlebots#starting-the-hardware-drivers]].
1. Now open a terminal and configure it to talk to the turtlebot via ROS as described in [[Turtlebots#configuring-your-local-pc-environment-for-accessing-the-real-turtlebot]]. Use this terminal (once configured) to try out a new command: `rostopic`. 
  1. First run `rostopic -h` in your terminal to see all the options it has (`-h` is always a good idea for any command in a terminal, sometimes it needs `--help` instead). 
  1. Run `rostopic list` and discuss with your neighbour what you see. Also try `rostopic echo /odom` and push the robot about. What do you see?
  1. Then run `rostopic pub -h` to get more detail about the sub-command `pub`. 
  1. Finally, we'll use `rostopic pub` to make the robot move (copy the following in to your terminal): 

    ```
rostopic pub -r 1 /cmd_vel geometry_msgs/Twist "
linear:
  x: 0.0
  y: 0.0
  z: 0.0
angular:
  x: 0.0
  y: 0.0
  z: 0.2"
    ```
    (You can stop this command by hitting `[Ctrl]-C` on your keyboard!)

  1. Watch your turtlebot move. Explain (to a demonstrator) what is happening. Try to make it go around in a circle by modifying the above command.

1. In your terminal run `rosrun rviz rviz`. Find out how to display different sensory information of the robots: Pointcloud, "laserscans" (discuss what this might actually be), and images. Show the demonstrator your how your turtlebot sees the world.

1. Time to code our first Python program using ROS: [[First Turtlebot Coding]]. The goal is (again) have the turtlebot first go around in a circle, this time from the Python program! When you have accomplished this, the next challenge for you is to make it go in the shape of a square of 1m length each side. Show the demonstrators your code working!

Good luck!

### Week 3: Computer Vision with ROS and OpenCV

* You have two tasks to engage in today. 
  1. The first one is to try [[Creating a reactive behaviour for the Turtlebot]]. The objective of this task is to have you implement a first "reactive" robot that takes input (from a simulated laser sensor, see lecture notes) and makes the robot move (in the same way as last week, by publishing to `/cmd_vel`) depending on the input.
  1. You shall try write Python code to read images from your robot, display them using OpenCV methods, and try out colour slicing as presented in the lecture to segment a coloured object of your choice, either in simulation or in reality. If you  try this in simulation, try to segment the brightly green-coloured box located on top of each simulated robot. Try to find suitable parameters to robustly segment that blob and show the output to the demonstrators. Take [`color_contours.py`] (https://github.com/LCAS/teaching/blob/indigo-devel/cmp3641m-code-fragments/scripts/color_contours.py) as a starting point for your implementation.
  * You will get your bonus mark if you successfully engaged in these two task.
* Read through this collection of useful resources beyond what has been presented in the lecture in B3: [[OpenCV and ROS]]

### Week 4: Braitenberg Vehicle
This week, we will be implementing Braitenberg Vehicles "abusing" the camera as a simple light sensor. The task is detailed in [[Braitenberg vehicles]]. The learning objectives here are to
* be able to subscribe to images and analyse the pixels in the retrieved image
* to send control commands to the robot based on the image content

